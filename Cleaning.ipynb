{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703a2baf-e5a4-4094-a0b2-e1383e9f14e0",
   "metadata": {},
   "source": [
    "# 原始数据清理及SQL数据导入准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52bca457-32f4-4bd4-abb8-f1fb678f1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aab526-3494-4e55-ab8e-93595b75220f",
   "metadata": {},
   "source": [
    "--> 检验数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc6fb03-3569-423f-8b5f-ff7379def548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists: True\n",
      "size: 389951705 bytes\n",
      "cwd: /Users/zhaolong/Documents/<freshgrad/SQL/project\n",
      "b'[\\n  {\\n    \"movie_id\": 1292052,\\n    \"title\": \"\\xe8\\x82\\x96\\xe7\\x94\\xb3\\xe5\\x85\\x8b\\xe7\\x9a\\x84\\xe6\\x95\\x91\\xe8\\xb5\\x8e\",\\n    \"original_title\": \"The Shawshank Redemption\",\\n    \"aka\": [\\n      \"[\\'\\xe6\\x9c\\x88\\xe9\\xbb\\x91\\xe9\\xab\\x98\\xe9\\xa3\\x9e(\\xe6\\xb8\\xaf)\\'\",\\n      \"\\'\\xe5\\x88\\xba\\xe6\\xbf\\x801995(\\xe5\\x8f\\xb0)\\'\",\\n      \"\\'\\xe5\\x9c\\xb0\\xe7\\x8b\\xb1'\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "\n",
    "path = \"douban_movies_2021.json\"\n",
    "print(\"exists:\", os.path.exists(path))\n",
    "print(\"size:\", os.path.getsize(path), \"bytes\")\n",
    "print(\"cwd:\", pathlib.Path().resolve())\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    head = f.read(200)\n",
    "print(head[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39cb962-dcb9-4af4-a94b-63b22c898280",
   "metadata": {},
   "source": [
    "## 1. 原始数据导入及主表构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "886e1346-c8cc-407c-a806-3cafcf1584e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_json('douban_movies_2021.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab569d3-29f0-4699-8eeb-9086fc3fbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_cols = [\n",
    "    'movie_id', 'title', 'original_title',\n",
    "    'year', 'mainland_pubdate', 'summary', 'record_time'\n",
    "]\n",
    "movie_df = raw_df[movie_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44c211a2-b0b8-48c4-82d9-d08fb873e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.to_csv('movie.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f67e7c-b2e4-439d-bea1-a843989f072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rating(r):\n",
    "    if pd.isna(r):\n",
    "        return pd.Series({\n",
    "            'score': None,\n",
    "            'rating_count': None,\n",
    "            'star_1_count': None,\n",
    "            'star_2_count': None,\n",
    "            'star_3_count': None,\n",
    "            'star_4_count': None,\n",
    "            'star_5_count': None\n",
    "        })\n",
    "    score = r.get('score')\n",
    "    count = r.get('count')\n",
    "    details = (r.get('raw') or {}).get('details', {})\n",
    "    return pd.Series({\n",
    "        'score': score,\n",
    "        'rating_count': count,\n",
    "        'star_1_count': int(details.get('1', 0)),\n",
    "        'star_2_count': int(details.get('2', 0)),\n",
    "        'star_3_count': int(details.get('3', 0)),\n",
    "        'star_4_count': int(details.get('4', 0)),\n",
    "        'star_5_count': int(details.get('5', 0)),\n",
    "    })\n",
    "\n",
    "rating_df = raw_df['rating'].apply(extract_rating)\n",
    "movie_rating_df = pd.concat([raw_df['movie_id'], rating_df], axis=1)\n",
    "\n",
    "# 再加上 collect_count / wish_count / comments_count / reviews_count\n",
    "for col in ['collect_count', 'wish_count',\n",
    "            'comments_count', 'reviews_count']:\n",
    "    movie_rating_df[col] = raw_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fa45380-5619-413e-afc1-236a43cfba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_df.to_csv('movie_rating.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310ea80-093e-4461-bbcf-69af9692a1f9",
   "metadata": {},
   "source": [
    "## 2. 电影类型维度表与关系表的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccca26bb-383a-4a12-9ee3-d82c04b73678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 先展开 genres（如果本身就是 list，这一步即可；如果是字符串后面再说）\n",
    "genre_exploded = (\n",
    "    raw_df[['movie_id', 'genres']]\n",
    "      .explode('genres')\n",
    "      .dropna(subset=['genres'])\n",
    ")\n",
    "\n",
    "# 2. 定义一个清洗函数，统一去掉 [], 引号 和多余空格\n",
    "def clean_genre(g):\n",
    "    if pd.isna(g):\n",
    "        return None\n",
    "    s = str(g).strip()\n",
    "    # 去掉最常见的包裹字符：[ ] ' \"\n",
    "    s = s.strip(\"[]'\\\" \")          # 先从两端剥一层\n",
    "    s = s.replace(\"[\", \"\").replace(\"]\", \"\")  # 再把中间残留的彻底去掉\n",
    "    s = s.strip(\" '\\\"\\t\\r\\n\")      # 再次去掉空白和引号\n",
    "    # 连续空白压缩为一个空格（防止中英文之间有多空格）\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s or None\n",
    "\n",
    "genre_exploded[\"genre_clean\"] = genre_exploded[\"genres\"].apply(clean_genre)\n",
    "# 丢掉被清洗成空或 None 的\n",
    "genre_exploded = genre_exploded.dropna(subset=[\"genre_clean\"])\n",
    "\n",
    "# 3. 用清洗后的字段生成维度表\n",
    "genre_dim = (\n",
    "    genre_exploded[[\"genre_clean\"]]\n",
    "      .drop_duplicates()\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "genre_dim[\"genre_id\"] = genre_dim.index + 1\n",
    "genre_dim = genre_dim.rename(columns={\"genre_clean\": \"genre_name\"})\n",
    "\n",
    "# 4. 生成关系表 movie_genre\n",
    "movie_genre_df = genre_exploded.merge(\n",
    "    genre_dim,\n",
    "    left_on=\"genre_clean\", right_on=\"genre_name\"\n",
    ")[[\"movie_id\", \"genre_id\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e974b92-8c71-4adf-89fa-c8cee7280b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dim.to_csv('genre.csv', index=False)\n",
    "movie_genre_df.to_csv('movie_genre.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25712b6-2b28-46ee-98fe-a2ac96760099",
   "metadata": {},
   "source": [
    "## 3. 电影国家/地区、语言维度表与关系表的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfbb6149-8f40-4089-b242-14a99802ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 专门给国家字段用的拆分+清洗函数\n",
    "def split_country_tokens(val):\n",
    "    \"\"\"\n",
    "    输入：countries 列中的一个元素（可能是 str / list 的单元）\n",
    "    输出：一个“干净的国家名”列表\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    # 去掉最外层 [], 引号 等\n",
    "    s = s.strip(\"[]'\\\" \")\n",
    "    s = s.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    s = s.strip()\n",
    "\n",
    "    # 如果整体看起来像日期或纯数字，直接丢弃\n",
    "    # 例：1958-06-29, 2019, 1997-1-1 等\n",
    "    if re.fullmatch(r\"\\d{4}(-\\d{1,2}(-\\d{1,2})?)?\", s):\n",
    "        return []\n",
    "\n",
    "    # 先按常见分隔符切一层：逗号 / 中文逗号 / 顿号 / 斜杠\n",
    "    parts = re.split(r\"[、,，/]+\", s)\n",
    "    tokens = []\n",
    "\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "\n",
    "        # 如果这一段里既有中文又有英文，或者是多段中文中间有空格，\n",
    "        # 说明其实是多个国家拼在一起，再按空格拆\n",
    "        if (re.search(r\"[\\u4e00-\\u9fff]\", part) and \" \" in part) or \\\n",
    "           (re.fullmatch(r\"[\\u4e00-\\u9fff\\s]+\", part) and \" \" in part):\n",
    "            sub_parts = [p.strip() for p in part.split(\" \") if p.strip()]\n",
    "            tokens.extend(sub_parts)\n",
    "        else:\n",
    "            # 对全英文的部分，不再按空格拆（避免把 \"United States\" 拆成两块）\n",
    "            tokens.append(part)\n",
    "\n",
    "    # 最后再统一清理一遍（去掉多余空白）\n",
    "    cleaned = []\n",
    "    for t in tokens:\n",
    "        t = t.strip(\" '\\\"\\t\\r\\n\")\n",
    "        t = re.sub(r\"\\s+\", \" \", t)  # 连续空白压缩成一个空格\n",
    "        if t:\n",
    "            cleaned.append(t)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# =============== 构造 country_exploded ===============\n",
    "\n",
    "rows = []\n",
    "for _, row in raw_df[['movie_id', 'countries']].iterrows():\n",
    "    mid = row['movie_id']\n",
    "    # countries 字段一般是 list，如果是单个字符串也兼容\n",
    "    vals = row['countries']\n",
    "    if isinstance(vals, (list, tuple)):\n",
    "        src_list = vals\n",
    "    else:\n",
    "        src_list = [vals]\n",
    "\n",
    "    for v in src_list:\n",
    "        for token in split_country_tokens(v):\n",
    "            rows.append({'movie_id': mid, 'country_clean': token})\n",
    "\n",
    "country_exploded = pd.DataFrame(rows)\n",
    "\n",
    "# 去掉同一部电影里重复的国家\n",
    "country_exploded = country_exploded.drop_duplicates(\n",
    "    subset=['movie_id', 'country_clean']\n",
    ")\n",
    "\n",
    "# =============== 维度表 country_dim ===============\n",
    "\n",
    "country_dim = (\n",
    "    country_exploded[['country_clean']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "country_dim['country_id'] = country_dim.index + 1\n",
    "country_dim = country_dim.rename(columns={'country_clean': 'country_name'})\n",
    "\n",
    "# =============== 关系表 movie_country_df ===============\n",
    "\n",
    "movie_country_df = country_exploded.merge(\n",
    "    country_dim,\n",
    "    left_on='country_clean', right_on='country_name'\n",
    ")[['movie_id', 'country_id']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05f90983-07bd-4e52-9d8b-9b931c14b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dim.to_csv('country.csv', index=False)\n",
    "movie_country_df.to_csv('movie_country.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae609192-136f-45a0-9246-f07b941bfaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 专门给 languages 用的拆分+清洗函数\n",
    "def split_language_tokens(val):\n",
    "    \"\"\"\n",
    "    输入：languages 列中的一个元素（可能是 str / list 的单元）\n",
    "    输出：一个“干净的语言名”列表\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    # 去掉最外层 [], 引号 等\n",
    "    s = s.strip(\"[]'\\\" \")\n",
    "    s = s.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    s = s.strip()\n",
    "\n",
    "    # 如果整体看起来像日期或纯数字，直接丢弃\n",
    "    # 例：1958-06-29, 2019, 1997-1-1 等\n",
    "    if re.fullmatch(r\"\\d{4}(-\\d{1,2}(-\\d{1,2})?)?\", s):\n",
    "        return []\n",
    "\n",
    "    # 按常见分隔符切一层：逗号 / 中文逗号 / 顿号 / 斜杠 / 分号 / 竖线\n",
    "    parts = re.split(r\"[、,，/;|]+\", s)\n",
    "    tokens = []\n",
    "\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "\n",
    "        # 如果这一段只包含中文和空格，且内部有空格，\n",
    "        # 说明很可能是多个中文语言名拼在一起，例如 \"法语 西班牙语\"\n",
    "        if re.fullmatch(r\"[\\u4e00-\\u9fff\\s]+\", part) and \" \" in part:\n",
    "            sub_parts = [p.strip() for p in part.split(\" \") if p.strip()]\n",
    "            tokens.extend(sub_parts)\n",
    "        else:\n",
    "            # 对英文多词语言名（含字母）或已经干净的中文名，不再按空格拆\n",
    "            tokens.append(part)\n",
    "\n",
    "    # 最后统一清理一次：去掉多余空白、引号等\n",
    "    cleaned = []\n",
    "    for t in tokens:\n",
    "        t = t.strip(\" '\\\"\\t\\r\\n\")\n",
    "        t = re.sub(r\"\\s+\", \" \", t)  # 连续空白压成一个空格\n",
    "        if t:\n",
    "            cleaned.append(t)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# =============== 构造 language_exploded ===============\n",
    "\n",
    "lang_rows = []\n",
    "for _, row in raw_df[['movie_id', 'languages']].iterrows():\n",
    "    mid = row['movie_id']\n",
    "    vals = row['languages']\n",
    "\n",
    "    # languages 字段有时是 list，有时是单个字符串，这里统一成 list\n",
    "    if isinstance(vals, (list, tuple)):\n",
    "        src_list = vals\n",
    "    else:\n",
    "        src_list = [vals]\n",
    "\n",
    "    for v in src_list:\n",
    "        for token in split_language_tokens(v):\n",
    "            lang_rows.append({'movie_id': mid, 'language_clean': token})\n",
    "\n",
    "language_exploded = pd.DataFrame(lang_rows)\n",
    "\n",
    "# 去掉同一电影里重复的语言\n",
    "language_exploded = language_exploded.drop_duplicates(\n",
    "    subset=['movie_id', 'language_clean']\n",
    ")\n",
    "\n",
    "# =============== 维度表 language_dim ===============\n",
    "\n",
    "language_dim = (\n",
    "    language_exploded[['language_clean']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "language_dim['language_id'] = language_dim.index + 1\n",
    "language_dim = language_dim.rename(columns={'language_clean': 'language_name'})\n",
    "\n",
    "# =============== 关系表 movie_language_df ===============\n",
    "\n",
    "movie_language_df = language_exploded.merge(\n",
    "    language_dim,\n",
    "    left_on='language_clean', right_on='language_name'\n",
    ")[['movie_id', 'language_id']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db5194f8-6401-4769-b189-bb3230afc954",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dim.to_csv('language.csv', index=False)\n",
    "movie_language_df.to_csv('movie_language.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b373b-066d-461b-aa3d-764bda6b99db",
   "metadata": {},
   "source": [
    "## 4. 电影标签维度表与关系表的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcc76657-1674-4886-9b5c-128ba783b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tag(t):\n",
    "    if pd.isna(t):\n",
    "        return None\n",
    "    s = str(t).strip()\n",
    "    # 去掉类似 \"['经典'\" 这样的前缀/后缀\n",
    "    s = s.strip(\"[]'\\\" \")\n",
    "    # 过滤纯数字（年份等）\n",
    "    if re.fullmatch(r'\\d{2,4}', s):\n",
    "        return None\n",
    "    return s or None\n",
    "\n",
    "tag_exploded = (\n",
    "    raw_df[['movie_id', 'tags']]\n",
    "      .explode('tags')\n",
    "      .dropna(subset=['tags'])\n",
    ")\n",
    "\n",
    "tag_exploded['tag_clean'] = tag_exploded['tags'].apply(clean_tag)\n",
    "tag_exploded = tag_exploded.dropna(subset=['tag_clean'])\n",
    "\n",
    "# 同一电影内部去重\n",
    "tag_exploded = tag_exploded.drop_duplicates(\n",
    "    subset=['movie_id', 'tag_clean']\n",
    ")\n",
    "\n",
    "# 构造 tag 维度表\n",
    "tag_dim = (\n",
    "    tag_exploded[['tag_clean']]\n",
    "      .drop_duplicates()\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "tag_dim['tag_id'] = tag_dim.index + 1\n",
    "tag_dim = tag_dim.rename(columns={'tag_clean': 'tag_name'})\n",
    "\n",
    "# 构造 movie_tag 关系表\n",
    "movie_tag_df = tag_exploded.merge(\n",
    "    tag_dim,\n",
    "    left_on='tag_clean', right_on='tag_name'\n",
    ")[['movie_id', 'tag_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b616db3d-9a5b-45db-b9fd-54070ea8bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dim.to_csv('tag.csv', index=False)\n",
    "movie_tag_df.to_csv('movie_tag.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4ccee-903f-419d-b2bb-db868f2d0b98",
   "metadata": {},
   "source": [
    "## 5. 电影外部播放平台维度表和关系表的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18a0fc6c-a00f-43a1-adef-abeb10a58f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_rows = []\n",
    "\n",
    "for _, row in raw_df.iterrows():\n",
    "    mid = row['movie_id']\n",
    "    for v in (row.get('videos') or []):\n",
    "        src = v.get('source') or {}\n",
    "        platform_rows.append({\n",
    "            'movie_id': mid,\n",
    "            'platform_literal': src.get('literal'),\n",
    "            'platform_name': src.get('name'),\n",
    "            'need_pay': v.get('need_pay'),\n",
    "            'sample_link': v.get('sample_link')\n",
    "        })\n",
    "\n",
    "platform_df = pd.DataFrame(platform_rows)\n",
    "\n",
    "# 构造 platform 维度表\n",
    "platform_dim = (\n",
    "    platform_df[['platform_literal', 'platform_name']]\n",
    "      .drop_duplicates()\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "platform_dim['platform_id'] = platform_dim.index + 1\n",
    "\n",
    "# 构造 movie_platform 关系表\n",
    "movie_platform_df = platform_df.merge(\n",
    "    platform_dim,\n",
    "    on=['platform_literal', 'platform_name']\n",
    ")[['movie_id', 'platform_id', 'need_pay', 'sample_link']]\n",
    "movie_platform_df = movie_platform_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c40683f-e011-4d4a-9cba-226ff00e7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_dim.to_csv('platform.csv', index=False)\n",
    "movie_platform_df.to_csv('movie_platform.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
